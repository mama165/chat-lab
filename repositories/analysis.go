//go:generate go run go.uber.org/mock/mockgen -source=analysis.go -destination=../mocks/mock_analysis_repository.go -package=mocks
package repositories

import (
	"chat-lab/domain"
	pb "chat-lab/proto/storage"
	"context"
	"fmt"
	"github.com/blugelabs/bluge"
	"github.com/dgraph-io/badger/v4"
	"github.com/google/uuid"
	"github.com/samber/lo"
	"google.golang.org/protobuf/proto"
	"google.golang.org/protobuf/types/known/timestamppb"
	"log/slog"
	"strings"
	"time"
)

type IAnalysisRepository interface {
	Store(analysis Analysis) error
	StoreBatch(analyses []Analysis) error
	ScanAnalysesByRoom(roomID string, cursor *string) ([]Analysis, *string, error)
	FetchFullByMessageId(roomID string, messageId uuid.UUID) (Analysis, error)
	SearchPaginated(ctx context.Context, query string, roomID string, offset int) ([]Analysis, uint64, error)
	SearchByScoreRange(ctx context.Context, scoreName string, min, max float64, roomID string) ([]Analysis, uint64, error)
	DeleteByMessageId(roomID string, messageId uuid.UUID) error
	Flush() error
}

type AnalysisRepository struct {
	db          *badger.DB
	writer      *bluge.Writer
	log         *slog.Logger
	badgerLimit *int
	blugeLimit  int
}

func NewAnalysisRepository(db *badger.DB, writer *bluge.Writer,
	log *slog.Logger, badgerLimit *int, blugeLimit int) *AnalysisRepository {
	return &AnalysisRepository{
		db: db, writer: writer,
		log: log, badgerLimit: badgerLimit,
		blugeLimit: blugeLimit,
	}
}

// Analysis represents the enriched metadata generated by the AI pipeline for a message.
// It acts as a bridge between specialized AI models (toxicity, sentiment, business)
// and the storage layer, supporting multi-modal payloads.
//
// Key design points:
//   - ID: Unique identifier for the analysis record itself.
//   - MessageId: Links the analysis to the original chat message.
//   - RoomId: Used as the primary partition key in BadgerDB for efficient prefix scans.
//   - At: Timestamp used to maintain reverse chronological order in the "War Room" UI.
//   - Scores: Map of technical metrics (e.g., toxicity: 0.98) aligned with Protobuf float64/double.
//   - Payload: A flexible container for specialized data types (TextContent, AudioDetails, or FileDetails).
//   - Version: Tracks which AI model version produced this data for schema evolution management.
type Analysis struct {
	ID        uuid.UUID
	MessageId uuid.UUID
	RoomId    string
	At        time.Time
	Summary   string
	Tags      []string
	Scores    map[domain.AnalysisMetric]float64
	Payload   any
	Version   uuid.UUID
}

type TextContent struct {
	Content string
}

type AudioDetails struct {
	Transcription string
	Duration      uint32
}

type FileDetails struct {
	Filename string
	MimeType string
	Size     uint64
}

// Store persists the analysis in BadgerDB and indexes its content in Bluge.
// This dual-write approach ensures that the data is both durable (Source of Truth)
// and searchable in real-time. The method dynamically selects the most relevant
// content to index based on the payload type (Text, Audio, or File).
func (a *AnalysisRepository) Store(analysis Analysis) error {
	key := buildKey(analysis.RoomId, analysis.At, analysis.MessageId)

	bytes, err := proto.Marshal(fromAnalysis(analysis))
	if err != nil {
		return err
	}

	err = a.db.Update(func(txn *badger.Txn) error {
		return txn.Set(key, bytes)
	})
	if err != nil {
		return fmt.Errorf("badger storage failed: %w", err)
	}

	// 1. Combine Summary + Payload Content for full-text search
	fullSearchableText := analysis.Summary
	var category domain.DataType

	switch p := analysis.Payload.(type) {
	case TextContent:
		fullSearchableText += " " + p.Content
		category = domain.TextType
	case AudioDetails:
		fullSearchableText += " " + p.Transcription
		category = domain.AudioType
	case FileDetails:
		fullSearchableText += " " + p.Filename
		category = domain.FileType
	default:
		a.log.Error("Unexpected analyse type", "type", analysis.Payload)
	}

	id := analysis.MessageId.String()
	return a.upsertSearchIndex(id, fullSearchableText, analysis.Scores, category, analysis.RoomId)
}

// upsertSearchIndex maps and persists analysis metadata into the Bluge search engine.
// It handles text tokenization for the content, exact matching for identifiers (room_id, type),
// and stores AI specialist scores as numeric fields to enable range-based filtering.
// This method ensures the search index stays synchronized with the primary BadgerDB storage.
func (a *AnalysisRepository) upsertSearchIndex(id, content string,
	metadata map[domain.AnalysisMetric]float64,
	category domain.DataType, roomID string) error {
	doc := bluge.NewDocument(id)
	// TextField allows full-text search with tokenization (e.g., lowercase, stop words)
	doc.AddField(bluge.NewTextField("content", content).StoreValue())
	// KeywordField stores strings as-is for exact matching (IDs, enums)
	doc.AddField(bluge.NewKeywordField("room_id", roomID).StoreValue())
	doc.AddField(bluge.NewKeywordField("type", string(category)).StoreValue())

	// NumericField allows optimized range queries (e.g., toxicity > 0.8)
	for name, score := range metadata {
		// We use the score name as the field name (e.g., "business", "toxicity")
		fieldName := strings.ToLower(string(name))
		doc.AddField(bluge.NewNumericField(fieldName, score).StoreValue())
	}

	// Update performs an upsert: it replaces the document if the ID already exists
	return a.writer.Update(doc.ID(), doc)
}

// StoreBatch persists multiple analysis records in BadgerDB using a WriteBatch for performance.
// Note: This only updates BadgerDB; the search index must be updated separately or via a loop.
func (a *AnalysisRepository) StoreBatch(analyses []Analysis) error {
	wb := a.db.NewWriteBatch()
	defer wb.Cancel()

	for _, anal := range analyses {
		analysisPb := fromAnalysis(anal)
		bytes, err := proto.Marshal(analysisPb)
		if err != nil {
			return err
		}

		key := buildKey(anal.RoomId, anal.At, anal.MessageId)
		if err := wb.Set(key, bytes); err != nil {
			return err
		}
	}
	return wb.Flush()
}

// ScanAnalysesByRoom retrieves a paginated list of analyses for a specific room using Badger's prefix scan.
// It iterates in reverse chronological order (the newest first) to support real-time monitoring UIs.
// Pagination is handled via a cursor which represents the relative part of the Badger key.
func (a *AnalysisRepository) ScanAnalysesByRoom(roomID string, cursor *string) ([]Analysis, *string, error) {
	var analyses []Analysis
	var lastKey string
	var hasMore bool

	err := a.db.View(func(txn *badger.Txn) error {
		prefixStr := fmt.Sprintf("analysis:%s:", roomID)
		prefix := []byte(prefixStr)
		prefixLen := len(prefixStr)

		opts := badger.DefaultIteratorOptions
		opts.Reverse = true
		it := txn.NewIterator(opts)
		defer it.Close()

		var seekKey []byte
		if cursor == nil {
			seekKey = append(prefix, []byte("9999999999999999999")...)
		} else {
			seekKey = append(prefix, []byte(*cursor)...)
		}

		it.Seek(seekKey)

		if cursor != nil && it.ValidForPrefix(prefix) {
			it.Next()
		}

		for ; it.ValidForPrefix(prefix); it.Next() {
			if a.badgerLimit != nil && len(analyses) >= *a.badgerLimit {
				hasMore = true
				break
			}

			item := it.Item()
			lastKey = string(item.Key()[prefixLen:])

			err := item.Value(func(v []byte) error {
				var pbAnal pb.Analysis
				if err := proto.Unmarshal(v, &pbAnal); err != nil {
					return err
				}

				anal, errMapping := toAnalysis(&pbAnal)
				if errMapping != nil {
					return errMapping
				}
				analyses = append(analyses, anal)
				return nil
			})
			if err != nil {
				return err
			}
		}
		return nil
	})

	if err != nil {
		return nil, nil, err
	}
	if len(analyses) == 0 {
		return analyses, nil, nil
	}

	if !hasMore {
		return analyses, nil, nil
	}

	return analyses, &lastKey, nil
}

// FetchFullByMessageId retrieves the complete Analysis object from BadgerDB.
// Currently, performs a prefix scan on the room, which is O(N).
// Optimization to O(1) via a secondary index is recommended for large datasets.
func (a *AnalysisRepository) FetchFullByMessageId(roomID string, messageId uuid.UUID) (Analysis, error) {
	var result Analysis
	prefix := []byte(fmt.Sprintf("analysis:%s:", roomID))
	msgIDStr := messageId.String()

	err := a.db.View(func(txn *badger.Txn) error {
		it := txn.NewIterator(badger.DefaultIteratorOptions)
		defer it.Close()

		for it.Seek(prefix); it.ValidForPrefix(prefix); it.Next() {
			item := it.Item()
			key := string(item.Key())

			// On cherche le messageId à la fin de la clé
			if len(key) >= len(msgIDStr) && key[len(key)-len(msgIDStr):] == msgIDStr {
				return item.Value(func(v []byte) error {
					var pbAnal pb.Analysis
					if err := proto.Unmarshal(v, &pbAnal); err != nil {
						return err
					}
					res, errMapping := toAnalysis(&pbAnal)
					result = res
					return errMapping
				})
			}
		}
		return fmt.Errorf("analysis not found for message: %s", messageId)
	})

	return result, err
}

// SearchPaginated performs a full-text search on the 'content' field, filtered by RoomId.
// It uses a MatchQuery for the content to handle tokenization and a TermQuery
// for the roomID to ensure an exact match on the KeywordField.
func (a *AnalysisRepository) SearchPaginated(ctx context.Context, query string, roomID string, offset int) ([]Analysis, uint64, error) {
	contentQuery := bluge.NewMatchQuery(query).SetField("content")

	var finalQuery bluge.Query
	if roomID != "" {
		bq := bluge.NewBooleanQuery()
		bq.AddMust(contentQuery)
		bq.AddMust(bluge.NewTermQuery(roomID).SetField("room_id"))
		finalQuery = bq
	} else {
		finalQuery = contentQuery
	}

	request := bluge.NewTopNSearch(a.blugeLimit, finalQuery).SetFrom(offset)

	// Pass the query to searchAndHydrate for counting
	return a.searchAndHydrate(ctx, request, finalQuery, roomID)
}

// SearchByScoreRange allows filtering messages based on AI specialist metrics (e.g., toxicity > 0.8).
func (a *AnalysisRepository) SearchByScoreRange(ctx context.Context, scoreName string, min, max float64, roomID string) ([]Analysis, uint64, error) {
	fieldName := strings.ToLower(scoreName)
	query := bluge.NewNumericRangeInclusiveQuery(min, max, true, true).SetField(fieldName)

	bq := bluge.NewBooleanQuery()
	bq.AddMust(query)
	if roomID != "" {
		bq.AddMust(bluge.NewTermQuery(roomID).SetField("room_id"))
	}

	request := bluge.NewTopNSearch(a.blugeLimit, bq)

	// Pass the query to searchAndHydrate for counting
	return a.searchAndHydrate(ctx, request, bq, roomID)
}

// DeleteByMessageId removes an analysis record.
// Implementation should handle both BadgerDB and Bluge index removal.
func (a *AnalysisRepository) DeleteByMessageId(roomID string, messageId uuid.UUID) error {
	// TODO: Implement dual-delete
	return nil
}

// Flush A batch operation in Bluge triggers a physical write/commit
func (a *AnalysisRepository) Flush() error {
	return a.writer.Batch(bluge.NewBatch())
}

// searchAndHydrate executes a Bluge search request and hydrates the results from BadgerDB.
// It bridges the gap between the lightweight search index and the full data store.
func (a *AnalysisRepository) searchAndHydrate(ctx context.Context, request bluge.SearchRequest, query bluge.Query, roomID string) ([]Analysis, uint64, error) {
	reader, err := a.writer.Reader()
	if err != nil {
		return nil, 0, fmt.Errorf("failed to open bluge reader: %w", err)
	}
	defer reader.Close()

	// STEP 1: Get exact total count using AllMatches
	// This is lightweight - it only iterates document IDs without loading content
	countRequest := bluge.NewAllMatches(query)
	dmiCount, err := reader.Search(ctx, countRequest)
	if err != nil {
		return nil, 0, fmt.Errorf("bluge count query failed: %w", err)
	}

	totalCount := uint64(0)
	match, err := dmiCount.Next()
	for err == nil && match != nil {
		totalCount++
		match, err = dmiCount.Next()
	}
	if err != nil {
		return nil, 0, fmt.Errorf("count iterator failed: %w", err)
	}

	// STEP 2: Execute the paginated search
	dmi, err := reader.Search(ctx, request)
	if err != nil {
		return nil, 0, fmt.Errorf("bluge search execution failed: %w", err)
	}

	var results []Analysis
	match, err = dmi.Next()

	// Main iteration loop - hydrate results from BadgerDB
	for err == nil && match != nil {
		var idStr string

		visitErr := match.VisitStoredFields(func(field string, value []byte) bool {
			if field == "_id" {
				idStr = string(value)
			}
			return true
		})
		if visitErr != nil {
			a.log.Error("failed to visit stored fields", "error", visitErr)
			match, err = dmi.Next()
			continue
		}

		msgID, parseErr := uuid.Parse(idStr)
		if parseErr != nil {
			a.log.Error("invalid uuid in search index", "id", idStr, "error", parseErr)
			match, err = dmi.Next()
			continue
		}

		fullAnalysis, fetchErr := a.FetchFullByMessageId(roomID, msgID)
		if fetchErr != nil {
			a.log.Warn("hydration_failed", "msg_id", msgID, "error", fetchErr)
		} else {
			results = append(results, fullAnalysis)
		}

		match, err = dmi.Next()
	}

	if err != nil {
		a.log.Error("bluge iterator encountered a fatal error", "error", err)
		return results, uint64(len(results)), fmt.Errorf("search iterator failed: %w", err)
	}

	a.log.Debug("search execution completed",
		"total_matches", totalCount,
		"results_count", len(results),
		"room_id", roomID,
	)

	return results, totalCount, nil
}

// buildKey: analysis:{room_id}:{timestamp_padded}:{message_id}
func buildKey(roomID string, at time.Time, messageID uuid.UUID) []byte {
	return []byte(fmt.Sprintf("analysis:%s:%019d:%s",
		roomID,
		at.UnixNano(),
		messageID.String(),
	))
}

func fromAnalysis(analysis Analysis) *pb.Analysis {
	scores := lo.MapKeys(analysis.Scores, func(_ float64, key domain.AnalysisMetric) string {
		return string(key)
	})

	res := &pb.Analysis{
		Id:        analysis.ID.String(),
		MessageId: analysis.MessageId.String(),
		RoomId:    analysis.RoomId,
		At:        timestamppb.New(analysis.At),
		Summary:   analysis.Summary,
		Tags:      analysis.Tags,
		Scores:    scores,
	}

	switch p := analysis.Payload.(type) {
	case TextContent:
		res.Payload = &pb.Analysis_TextContent{
			TextContent: &pb.TextContent{Content: p.Content},
		}
	case AudioDetails:
		res.Payload = &pb.Analysis_Audio{
			Audio: &pb.AudioDetails{
				Transcription: p.Transcription,
				DurationSec:   p.Duration,
			},
		}
	case FileDetails:
		res.Payload = &pb.Analysis_File{
			File: &pb.FileDetails{
				Filename: p.Filename,
				MimeType: p.MimeType,
				Size:     p.Size,
			},
		}
	}
	return res
}

func toAnalysis(analysisPb *pb.Analysis) (Analysis, error) {
	id, err := uuid.Parse(analysisPb.Id)
	if err != nil {
		return Analysis{}, err
	}
	messageID, err := uuid.Parse(analysisPb.MessageId)
	if err != nil {
		return Analysis{}, err
	}

	scores := lo.MapKeys(analysisPb.Scores, func(_ float64, key string) domain.AnalysisMetric {
		return domain.AnalysisMetric(key)
	})

	res := Analysis{
		ID:        id,
		MessageId: messageID,
		RoomId:    analysisPb.RoomId,
		At:        analysisPb.At.AsTime(),
		Summary:   analysisPb.Summary,
		Tags:      analysisPb.Tags,
		Scores:    scores,
	}

	if analysisPb.Payload != nil {
		switch p := analysisPb.Payload.(type) {
		case *pb.Analysis_TextContent:
			res.Payload = TextContent{Content: p.TextContent.Content}
		case *pb.Analysis_Audio:
			res.Payload = AudioDetails{
				Transcription: p.Audio.Transcription,
				Duration:      p.Audio.DurationSec,
			}
		case *pb.Analysis_File:
			res.Payload = FileDetails{
				Filename: p.File.Filename,
				MimeType: p.File.MimeType,
				Size:     p.File.Size,
			}
		}
	}
	return res, nil
}
