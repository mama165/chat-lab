//go:generate go run go.uber.org/mock/mockgen -source=analysis_repository.go -destination=../../mocks/mock_analysis_repository.go -package=mocks
package storage

import (
	"chat-lab/domain"
	pb "chat-lab/proto/storage"
	"context"
	"fmt"
	"log/slog"
	"strings"
	"time"

	"github.com/blugelabs/bluge"
	"github.com/dgraph-io/badger/v4"
	"github.com/google/uuid"
	"github.com/samber/lo"
	"google.golang.org/protobuf/proto"
	"google.golang.org/protobuf/types/known/timestamppb"
)

type IAnalysisRepository interface {
	Store(analysis Analysis) error
	StoreBatch(analyses []Analysis) error
	ScanAnalysesByRoom(namespace string, cursor *string) ([]Analysis, *string, error)
	FetchFullByEntityId(namespace string, entityID uuid.UUID) (Analysis, error)
	SearchPaginated(ctx context.Context, query string, namespace string, offset int) ([]Analysis, uint64, error)
	SearchByScoreRange(ctx context.Context, scoreName string, min, max float64, namespace string) ([]Analysis, uint64, error)
	Flush() error
}

type AnalysisRepository struct {
	db          *badger.DB
	writer      *bluge.Writer
	log         *slog.Logger
	badgerLimit *int
	blugeLimit  int
}

func NewAnalysisRepository(db *badger.DB, writer *bluge.Writer,
	log *slog.Logger, badgerLimit *int, blugeLimit int) *AnalysisRepository {
	return &AnalysisRepository{
		db: db, writer: writer,
		log: log, badgerLimit: badgerLimit,
		blugeLimit: blugeLimit,
	}
}

// Analysis represents the enriched metadata generated by the AI pipeline for any digital entity (Chat, File, Audio).
// It acts as a bridge between specialized AI models and the storage layer,
// using a Namespace-based partitioning strategy to simulate a multi-table environment.
//
// Key design points:
//   - ID: Unique identifier for the analysis record itself.
//   - EntityId: Global identifier for the analyzed object (e.g., MessageUUID, FilePathHash, or AssetID).
//   - Namespace: The primary partition key (Bucket). Use "chat:{room_id}" or "scan:{drive_id}"
//     to isolate data streams and optimize BadgerDB prefix scans.
//   - At: Timestamp used to maintain reverse chronological order and time-range filtering.
//   - Scores: Map of technical metrics (e.g., toxicity, risk_score) generated by specialists.
//   - Payload: A polymorphic container (TextContent, AudioDetails, or FileDetails).
//   - Version: Tracks the specific AI model/Specialist version for data lineage.
type Analysis struct {
	ID        uuid.UUID
	EntityId  uuid.UUID
	Namespace string
	At        time.Time
	Summary   string
	Tags      []string
	Scores    map[domain.Metric]float64
	Payload   any
	Version   uuid.UUID
}

type TextContent struct {
	Content string
}

type AudioDetails struct {
	Transcription string
	Duration      float64
}

type FileDetails struct {
	Filename string
	MimeType string
	Size     uint64
	Content  string
}

const (
	CONTENT   = "content"
	NAMESPACE = "namespace"
	TYPE      = "type"
)

// Store persists the analysis in BadgerDB and indexes its content in Bluge.
// This dual-write approach ensures that the data is both durable (Source of Truth)
// and searchable in real-time. The method dynamically selects the most relevant
// content to index based on the payload type (Text, Audio, or File).
func (a *AnalysisRepository) Store(analysis Analysis) error {
	mainKey := buildKey(analysis.Namespace, analysis.At, analysis.EntityId)
	// Create the secondary index pointer for O(1) lookup
	indexKey := []byte(fmt.Sprintf("idx:msg:%s", analysis.EntityId.String()))

	bytes, err := proto.Marshal(fromAnalysis(analysis))
	if err != nil {
		return err
	}

	// Store both the main data AND the secondary index pointer
	err = a.db.Update(func(txn *badger.Txn) error {
		if err := txn.Set(mainKey, bytes); err != nil {
			return err
		}
		// CRITICAL: Also store the index pointer (this was missing!)
		return txn.Set(indexKey, mainKey)
	})
	if err != nil {
		return fmt.Errorf("badger storage failed: %w", err)
	}

	// Extract searchable text based on payload type

	fullSearchableText, category := a.prepareInternalData(analysis)

	id := analysis.EntityId.String()
	return a.upsertSearchIndex(id, fullSearchableText, analysis.Scores, category, analysis.Namespace)
}

// upsertSearchIndex maps and persists analysis metadata into the Bluge search engine.
// It handles text tokenization for the content, exact matching for identifiers (namespace, type),
// and stores AI specialist scores as numeric fields to enable range-based filtering.
// This method ensures the search index stays synchronized with the primary BadgerDB storage.
func (a *AnalysisRepository) upsertSearchIndex(id, content string,
	metadata map[domain.Metric]float64,
	category domain.Category, namespace string) error {
	doc := bluge.NewDocument(id)
	// TextField allows full-text search with tokenization (e.g., lowercase, stop words)
	doc.AddField(bluge.NewTextField(CONTENT, content).StoreValue())
	// KeywordField stores strings as-is for exact matching (IDs, enums)
	doc.AddField(bluge.NewKeywordField(NAMESPACE, namespace).StoreValue())
	doc.AddField(bluge.NewKeywordField(TYPE, string(category)).StoreValue())

	// NumericField allows optimized range queries (e.g., toxicity > 0.8)
	for name, score := range metadata {
		// We use the score name as the field name (e.g., "business", "toxicity")
		fieldName := strings.ToLower(string(name))
		doc.AddField(bluge.NewNumericField(fieldName, score).StoreValue())
	}

	// Update performs an upsert: it replaces the document if the ID already exists
	return a.writer.Update(doc.ID(), doc)
}

// StoreBatch persists a collection of analysis records using a dual-write batching strategy.
// It simultaneously updates the BadgerDB primary store and the Bluge search index.
//
// Performance Design:
//   - BadgerDB: Uses a WriteBatch (wb) to group multiple LSM-tree insertions into a single
//     flush, minimizing disk synchronization cycles. It also stores a secondary index
//     pointer (idx:msg:{uuid}) to allow O(1) retrieval.
//   - Bluge Index: Groups multiple document updates into a single bluge.Batch. This
//     drastically reduces the CPU time spent in Syscall6 (I/O wait) by avoiding redundant
//     file descriptor locks and segment merges for each document.
//   - Hydration Strategy: Each document is indexed with its EntityId as the unique
//     identifier, allowing O(1) retrieval from the primary store during search results hydration.
//
// Error Handling:
//
//	The method ensures that both batches are prepared before any disk commit. If either
//	Badger or Bluge flush fails, an error is returned to the caller for retry or logging.
func (a *AnalysisRepository) StoreBatch(analyses []Analysis) error {
	wb := a.db.NewWriteBatch()
	defer wb.Cancel()

	blugeBatch := bluge.NewBatch()

	for _, analysis := range analyses {
		// --- Badger: Prepare Protobuf, Main Key and Index Key ---
		analysisPb := fromAnalysis(analysis)
		bytes, err := proto.Marshal(analysisPb)
		if err != nil {
			return fmt.Errorf("failed to marshal analysis %s: %w", analysis.EntityId, err)
		}

		mainKey := buildKey(analysis.Namespace, analysis.At, analysis.EntityId)
		// Pointer key for O(1) lookup: idx:msg:{uuid} -> mainKey
		indexKey := []byte(fmt.Sprintf("idx:msg:%s", analysis.EntityId.String()))

		a.log.Debug("üíæ WRITING TO BADGER",
			"key", string(mainKey),
			"entity_id", analysis.EntityId.String(),
			"namespace", analysis.Namespace)

		// Store both the data and the pointer in the same batch
		if err := wb.Set(mainKey, bytes); err != nil {
			return fmt.Errorf("failed to set badger main key: %w", err)
		}
		if err := wb.Set(indexKey, mainKey); err != nil {
			return fmt.Errorf("failed to set badger index pointer: %w", err)
		}

		fullSearchableText, category := a.prepareInternalData(analysis)

		doc := bluge.NewDocument(analysis.EntityId.String())
		doc.AddField(bluge.NewTextField(CONTENT, fullSearchableText).StoreValue())
		doc.AddField(bluge.NewKeywordField(NAMESPACE, analysis.Namespace).StoreValue())
		doc.AddField(bluge.NewKeywordField(TYPE, string(category)).StoreValue())

		for name, score := range analysis.Scores {
			fieldName := strings.ToLower(string(name))
			doc.AddField(bluge.NewNumericField(fieldName, score).StoreValue())
		}

		blugeBatch.Update(doc.ID(), doc)
	}

	// Finalize writes
	if err := wb.Flush(); err != nil {
		return fmt.Errorf("badger write batch flush failed: %w", err)
	}
	if err := a.writer.Batch(blugeBatch); err != nil {
		return fmt.Errorf("bluge batch execution failed: %w", err)
	}

	return nil
}

// ScanAnalysesByRoom retrieves a paginated list of analyses for a specific room using Badger's prefix scan.
// It iterates in reverse chronological order (the newest first) to support real-time monitoring UIs.
// Pagination is handled via a cursor which represents the relative part of the Badger key.
func (a *AnalysisRepository) ScanAnalysesByRoom(Namespace string, cursor *string) ([]Analysis, *string, error) {
	var analyses []Analysis
	var lastKey string
	var hasMore bool

	err := a.db.View(func(txn *badger.Txn) error {
		prefixStr := fmt.Sprintf("analysis:%s:", Namespace)
		prefix := []byte(prefixStr)
		prefixLen := len(prefixStr)

		opts := badger.DefaultIteratorOptions
		opts.Reverse = true
		it := txn.NewIterator(opts)
		defer it.Close()

		var seekKey []byte
		if cursor == nil {
			seekKey = append(prefix, []byte("9999999999999999999")...)
		} else {
			seekKey = append(prefix, []byte(*cursor)...)
		}

		it.Seek(seekKey)

		if cursor != nil && it.ValidForPrefix(prefix) {
			it.Next()
		}

		for ; it.ValidForPrefix(prefix); it.Next() {
			if a.badgerLimit != nil && len(analyses) >= *a.badgerLimit {
				hasMore = true
				break
			}

			item := it.Item()
			lastKey = string(item.Key()[prefixLen:])

			err := item.Value(func(v []byte) error {
				var pbAnalysis pb.Analysis
				if err := proto.Unmarshal(v, &pbAnalysis); err != nil {
					return err
				}

				an, errMapping := ToAnalysis(&pbAnalysis)
				if errMapping != nil {
					return errMapping
				}
				analyses = append(analyses, an)
				return nil
			})
			if err != nil {
				return err
			}
		}
		return nil
	})

	if err != nil {
		return nil, nil, err
	}
	if len(analyses) == 0 {
		return analyses, nil, nil
	}

	if !hasMore {
		return analyses, nil, nil
	}

	return analyses, &lastKey, nil
}

// FetchFullByEntityId retrieves the complete Analysis object from BadgerDB.
// It uses a two-step O(1) lookup strategy to avoid expensive prefix scans:
//  1. Lookup the main data key using a secondary index pointer (idx:msg:{uuid}).
//  2. Retrieve and unmarshal the actual Protobuf data from the main key. [cite: 2026-01-19]
//
// Security & Consistency:
//
//	Even though the UUID lookup is global, the method validates that the retrieved
//	record belongs to the requested Namespace to ensure data isolation. [cite: 2026-01-19]
func (a *AnalysisRepository) FetchFullByEntityId(namespace string, entityID uuid.UUID) (Analysis, error) {
	var result Analysis

	// The secondary index key acts as a "shortcut" to the real data location.
	indexKey := []byte(fmt.Sprintf("idx:msg:%s", entityID.String()))

	err := a.db.View(func(txn *badger.Txn) error {
		// STEP 1: Retrieve the pointer (the main key) from the secondary index.
		itemIdx, err := txn.Get(indexKey)
		if err != nil {
			return fmt.Errorf("analysis index not found for message %s: %w", entityID, err)
		}

		return itemIdx.Value(func(mainKey []byte) error {
			// STEP 2: Retrieve the actual Protobuf payload using the main key pointer.
			itemData, err := txn.Get(mainKey)
			if err != nil {
				return fmt.Errorf("analysis data not found for message %s: %w", entityID, err)
			}

			return itemData.Value(func(v []byte) error {
				var pbAnal pb.Analysis
				if err := proto.Unmarshal(v, &pbAnal); err != nil {
					return fmt.Errorf("failed to unmarshal analysis protobuf: %w", err)
				}

				// Map Protobuf to our domain Analysis struct.
				res, errMapping := ToAnalysis(&pbAnal)
				if errMapping != nil {
					return fmt.Errorf("failed to map protobuf to domain: %w", errMapping)
				}

				// STEP 3: Security Check. Verify the record belongs to the requested room.
				if res.Namespace != namespace {
					return fmt.Errorf("security violation: message %s does not belong to room %s", entityID, namespace)
				}

				result = res
				return nil
			})
		})
	})

	return result, err
}

// SearchPaginated performs a full-text search on the 'content' field, filtered by namespace.
// It uses a MatchQuery for the content to handle tokenization and a TermQuery
// for the namespace to ensure an exact match on the KeywordField.
func (a *AnalysisRepository) SearchPaginated(ctx context.Context, query string, namespace string, offset int) ([]Analysis, uint64, error) {
	contentQuery := bluge.NewMatchQuery(query).SetField(CONTENT)

	var finalQuery bluge.Query
	if namespace != "" {
		bq := bluge.NewBooleanQuery()
		bq.AddMust(contentQuery)
		bq.AddMust(bluge.NewTermQuery(namespace).SetField(NAMESPACE))
		finalQuery = bq
	} else {
		finalQuery = contentQuery
	}

	request := bluge.NewTopNSearch(a.blugeLimit, finalQuery).SetFrom(offset)

	// Pass the query to searchAndHydrate for counting
	return a.searchAndHydrate(ctx, request, finalQuery, namespace)
}

// SearchByScoreRange allows filtering messages based on AI specialist metrics (e.g., toxicity > 0.8).
func (a *AnalysisRepository) SearchByScoreRange(ctx context.Context,
	scoreName string, min,
	max float64, Namespace string) ([]Analysis, uint64, error) {
	fieldName := strings.ToLower(scoreName)
	query := bluge.NewNumericRangeInclusiveQuery(min, max, true, true).SetField(fieldName)

	bq := bluge.NewBooleanQuery()
	bq.AddMust(query)
	if Namespace != "" {
		bq.AddMust(bluge.NewTermQuery(Namespace).SetField(NAMESPACE))
	}

	request := bluge.NewTopNSearch(a.blugeLimit, bq)

	// Pass the query to searchAndHydrate for counting
	return a.searchAndHydrate(ctx, request, bq, Namespace)
}

// Flush A batch operation in Bluge triggers a physical write/commit
func (a *AnalysisRepository) Flush() error {
	return a.writer.Batch(bluge.NewBatch())
}

// searchAndHydrate executes a Bluge search request and hydrates the results from BadgerDB.
// It bridges the gap between the lightweight search index and the full data store.
func (a *AnalysisRepository) searchAndHydrate(ctx context.Context, request bluge.SearchRequest, query bluge.Query, Namespace string) ([]Analysis, uint64, error) {
	reader, err := a.writer.Reader()
	if err != nil {
		return nil, 0, fmt.Errorf("failed to open bluge reader: %w", err)
	}
	defer reader.Close()

	// STEP 1: Get exact total count using AllMatches
	// This is lightweight - it only iterates document IDs without loading content
	countRequest := bluge.NewAllMatches(query)
	dmiCount, err := reader.Search(ctx, countRequest)
	if err != nil {
		return nil, 0, fmt.Errorf("bluge count query failed: %w", err)
	}

	totalCount := uint64(0)
	match, err := dmiCount.Next()
	for err == nil && match != nil {
		totalCount++
		match, err = dmiCount.Next()
	}
	if err != nil {
		return nil, 0, fmt.Errorf("count iterator failed: %w", err)
	}

	// STEP 2: Execute the paginated search
	dmi, err := reader.Search(ctx, request)
	if err != nil {
		return nil, 0, fmt.Errorf("bluge search execution failed: %w", err)
	}

	var results []Analysis
	match, err = dmi.Next()

	// Main iteration loop - hydrate results from BadgerDB
	for err == nil && match != nil {
		var idStr string

		visitErr := match.VisitStoredFields(func(field string, value []byte) bool {
			if field == "_id" {
				idStr = string(value)
			}
			return true
		})
		if visitErr != nil {
			a.log.Error("failed to visit stored fields", "error", visitErr)
			match, err = dmi.Next()
			continue
		}

		msgID, parseErr := uuid.Parse(idStr)
		if parseErr != nil {
			a.log.Error("invalid uuid in search index", "id", idStr, "error", parseErr)
			match, err = dmi.Next()
			continue
		}

		fullAnalysis, fetchErr := a.FetchFullByEntityId(Namespace, msgID)
		if fetchErr != nil {
			a.log.Warn("hydration_failed", "msg_id", msgID, "error", fetchErr)
		} else {
			results = append(results, fullAnalysis)
		}

		match, err = dmi.Next()
	}

	if err != nil {
		a.log.Error("bluge iterator encountered a fatal error", "error", err)
		return results, uint64(len(results)), fmt.Errorf("search iterator failed: %w", err)
	}

	a.log.Debug("search execution completed",
		"total_matches", totalCount,
		"results_count", len(results),
		"room_id", Namespace,
	)

	return results, totalCount, nil
}

// buildKey: analysis:{room_id}:{timestamp_padded}:{message_id}
func buildKey(Namespace string, at time.Time, EntityId uuid.UUID) []byte {
	return []byte(fmt.Sprintf("analysis:%s:%019d:%s",
		Namespace,
		at.UnixNano(),
		EntityId.String(),
	))
}

func fromAnalysis(analysis Analysis) *pb.Analysis {
	scores := lo.MapKeys(analysis.Scores, func(_ float64, key domain.Metric) string {
		return string(key)
	})

	res := &pb.Analysis{
		Id:        analysis.ID.String(),
		EntityId:  analysis.EntityId.String(),
		Namespace: analysis.Namespace,
		At:        timestamppb.New(analysis.At),
		Summary:   analysis.Summary,
		Tags:      analysis.Tags,
		Scores:    scores,
	}

	switch p := analysis.Payload.(type) {
	case TextContent:
		res.Payload = &pb.Analysis_TextContent{
			TextContent: &pb.TextContent{
				Content: p.Content,
			},
		}
	case *TextContent:
		res.Payload = &pb.Analysis_TextContent{
			TextContent: &pb.TextContent{
				Content: p.Content,
			},
		}
	case AudioDetails:
		res.Payload = &pb.Analysis_Audio{
			Audio: &pb.AudioDetails{
				Transcription: p.Transcription,
				DurationSec:   p.Duration,
			},
		}
	case *AudioDetails:
		res.Payload = &pb.Analysis_Audio{
			Audio: &pb.AudioDetails{
				Transcription: p.Transcription, DurationSec: p.Duration,
			},
		}
	case FileDetails:
		res.Payload = &pb.Analysis_File{
			File: &pb.FileDetails{
				Filename: p.Filename,
				MimeType: p.MimeType,
				Size:     p.Size,
				Content:  p.Content,
			},
		}
	case *FileDetails:
		res.Payload = &pb.Analysis_File{
			File: &pb.FileDetails{
				Filename: p.Filename,
				MimeType: p.MimeType,
				Size:     p.Size,
				Content:  p.Content,
			},
		}
	}
	return res
}

// ToAnalysis converts a Protobuf Analysis message to a domain Analysis struct.
// It handles UUID parsing and maps the polymorphic payload (Text, Audio, File).
func ToAnalysis(analysisPb *pb.Analysis) (Analysis, error) {
	// 1. Parse Identifiers
	id, err := uuid.Parse(analysisPb.Id)
	if err != nil {
		return Analysis{}, fmt.Errorf("invalid analysis id: %w", err)
	}
	entityId, err := uuid.Parse(analysisPb.EntityId)
	if err != nil {
		return Analysis{}, fmt.Errorf("invalid entity id: %w", err)
	}

	// Map string keys to domain Metric type
	scores := lo.MapKeys(analysisPb.Scores, func(_ float64, key string) domain.Metric {
		return domain.Metric(key)
	})

	// 3. Initialize Base Structure
	res := Analysis{
		ID:        id,
		EntityId:  entityId,
		Namespace: analysisPb.Namespace,
		At:        analysisPb.At.AsTime(),
		Summary:   analysisPb.Summary,
		Tags:      analysisPb.Tags,
		Scores:    scores,
	}

	// 4. Map Polymorphic Payload
	if analysisPb.Payload != nil {
		// Log the concrete type of the protobuf oneof for debugging purposes
		fmt.Printf("üîç [DEBUG MAPPER] Protobuf Payload Type: %T\n", analysisPb.Payload)

		switch p := analysisPb.Payload.(type) {
		case *pb.Analysis_TextContent:
			res.Payload = TextContent{
				Content: p.TextContent.Content,
			}

		case *pb.Analysis_Audio:
			res.Payload = AudioDetails{
				Transcription: p.Audio.Transcription,
				Duration:      p.Audio.DurationSec,
			}

		case *pb.Analysis_File:
			res.Payload = FileDetails{
				Filename: p.File.Filename,
				MimeType: p.File.MimeType,
				Size:     p.File.Size,
				Content:  p.File.Content,
			}

			// Even if it's a file, let's see what's inside the proto object
			fmt.Printf("üìÇ [DEBUG FILE] Content size: %d bytes\n", len(p.File.Content))
			fmt.Printf("üìÇ [DEBUG FILE] Raw Content: %s\n", p.File.Content)

		default:
			fmt.Printf("‚ö†Ô∏è [DEBUG MAPPER] Unknown payload type: %T\n", p)
		}
	}

	return res, nil
}

// prepareInternalData extracts searchable text and determines the data category
// from the flexible analysis payload. It handles both value and pointer types
// to ensure consistency between storage and search indexing.
func (a *AnalysisRepository) prepareInternalData(an Analysis) (string, domain.Category) {
	fullText := an.Summary
	category := domain.TextType // Default category

	if an.Payload == nil {
		return fullText, category
	}

	switch p := an.Payload.(type) {
	case TextContent:
		fullText += " " + p.Content
		category = domain.TextType
	case *TextContent:
		if p != nil {
			fullText += " " + p.Content
		}
		category = domain.TextType
	case AudioDetails:
		fullText += " " + p.Transcription
		category = domain.AudioType
	case *AudioDetails:
		if p != nil {
			fullText += " " + p.Transcription
		}
		category = domain.AudioType

	case FileDetails:
		fullText += " " + p.Filename
		if p.Content != "" {
			fullText += " " + p.Content
		}
		category = domain.FileType
	case *FileDetails:
		if p != nil {
			fullText += " " + p.Filename
			if p.Content != "" {
				fullText += " " + p.Content
			}
		}
		category = domain.FileType

	default:
		// Log the exact type to help debugging unexpected AI pipeline outputs [cite: 2026-01-19]
		a.log.Warn("unknown payload type during data preparation",
			"message_id", an.EntityId,
			"type", fmt.Sprintf("%T", an.Payload))
	}

	return fullText, category
}
