import sys
import os
import pandas as pd
import m2cgen as m2c
import numpy as np
from xgboost import XGBClassifier

# Needed for deep decision trees in m2cgen
sys.setrecursionlimit(10000)

def fnv1a_32(data):
    # Standard FNV-1a for Go compatibility
    hash_val = 0x811c9dc5
    for byte in str(data).encode('utf-8'):
        hash_val ^= byte
        hash_val = (hash_val * 0x01000193) & 0xFFFFFFFF
    return hash_val

def manual_vectorizer(texts, size=4096): # Increased to 4096 for better resolution
    vectors = []
    for text in texts:
        vec = np.zeros(size)
        # MUST match the Go 'unicode.IsPunct' logic as much as possible
        # Lowercasing + removing basic punctuation
        t = str(text).lower()
        for char in '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~':
            t = t.replace(char, "")

        words = t.split()
        for w in words:
            idx = fnv1a_32(w) % size
            vec[idx] += 1.0
        vectors.append(vec)
    return np.array(vectors)

def train_and_export():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    csv_path = os.path.join(current_dir, "train.csv")

    if not os.path.exists(csv_path):
        print(f"âŒ Dataset not found at {csv_path}")
        return

    print(f"--- ðŸ“– Loading {csv_path} ---")
    df_raw = pd.read_csv(csv_path)

    # Davidson mapping: 0=hate, 1=offensive -> 1 | 2=neither -> 0
    df_raw['label'] = df_raw['class'].apply(lambda x: 1 if x < 2 else 0)

    # Priority cases (the "unique" flavor of your chat)
    priority_data = pd.DataFrame([
        {'tweet': "naze", 'label': 1},
        {'tweet': "You are a complete idiot and I hate you", 'label': 1},
        {'tweet': "Shut up you loser", 'label': 1},
        {'tweet': "EspÃ¨ce de gros naze", 'label': 1},
        {'tweet': "Va mourir sale pourriture", 'label': 1},
        {'tweet': "Je ne suis pas d'accord avec toi", 'label': 0},
        {'tweet': "C'est de la balle, je tue le game", 'label': 0}, # Context test
        {'tweet': "Bonjour tout le monde", 'label': 0}
    ])

    target_sample = 5000
    df_toxic = df_raw[df_raw['label'] == 1].sample(n=min(target_sample, len(df_raw[df_raw['label']==1])), random_state=42)
    df_healthy = df_raw[df_raw['label'] == 0].sample(n=min(target_sample, len(df_raw[df_raw['label']==0])), random_state=42)

    # We repeat priority data to give it more "weight" in the brain of the AI
    df = pd.concat([df_toxic, df_healthy] + [priority_data]*100).sample(frac=1, random_state=42)

    print(f"--- ðŸ§  Training XGBoost on {len(df)} rows ---")
    X = manual_vectorizer(df['tweet'].values, size=1024)
    y = df['label'].values

    # XGBoost setup for m2cgen compatibility
    model = XGBClassifier(
        n_estimators=80,      # Number of trees (balance between size and smarts)
        max_depth=6,          # Depth of logic
        learning_rate=0.1,
        n_jobs=-1,             # Use all CPU cores for training
        base_score=0.5
    )
    model.fit(X, y)

    print("--- âš™ï¸ Exporting via m2cgen ---")
    # Exporting with a clear function name for Go
    code = m2c.export_to_go(model, function_name="PredictToxicity")

    output_path = "/app/model.go"
    with open(output_path, "w") as f:
        f.write("// Code generated by Chat-Lab AI-Factory. DO NOT EDIT.\n")
        f.write("package ai\n\n")
        f.write(code)
    print(f"âœ… model.go generated with success at {output_path}")

if __name__ == "__main__":
    train_and_export()